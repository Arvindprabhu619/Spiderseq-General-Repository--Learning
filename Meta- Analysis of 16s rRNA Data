Install and unzip sratoolkit

cd ~
wget https://ftp-trace.ncbi.nlm.nih.gov/sra/tools/sratoolkit.current-ubuntu64.tar.gz
tar -xvf sratoolkit.current-ubuntu64.tar.gz

#Find where the downloaded sratoolkit is located
find / -type d -name "sratoolkit*" 2>/dev/null

#Fixing the path for installed SRA toolkit, so that it could be accessed from anywhere in the terminal
echo 'export PATH="/home/spiseq/Desktop/sratoolkit.3.2.1-ubuntu64/bin:$PATH"' >> ~/.bashrc
source ~/.bashrc

#Download prepared metadata sheet csv

#Convert the csv to txt (only the sample run column) is requried for downloading
cut -d',' -f1 v3v4.csv > srr_v3v4.txt

#Check how it parsed and stored
head srr_v4v5.txt

#output
Run
SRR15609946

"

"
SRR15609947

"
#Then
sed -i '1{/^Run$/d}' srr_v4v5.txt
sed -i 's/"//g' srr_v4v5.txt
sed -i '/^$/d' srr_v4v5.txt
sed -i 's/^[ \t]*//;s/[ \t]*$//' srr_v4v5.txt

#Check how it parsed and stored
head srr_v4v5.txt

#Output
SRR15609946
SRR15609947
SRR15609948
SRR15609949
SRR15609950
SRR15609951
SRR15609952
SRR15609953
SRR15609954
SRR15609955

#Good to Go
#Go to the directory you want to store your data
cd ~/SRA_data/sra/raw_V4V5

#Prefetch to pull the data from NCBI
prefetch --output-directory /home/spiseq/SRA_data/sra/raw_V4V5/srr_v4v5_raw_files --option-file srr_v4v5.txt (parsed txt file-prepared by above method)

#The above comment downloads files inside folder (each file is loaded inside folder),to remove multiple folder make it come under one common folder
$ cd /home/spiseq/SRA_data/sra/raw_V3V4/srr_v3v4_raw_files
$ find . -type f -name "*.sra" -exec mv {} . \;
$ find . -type d -empty -delete
$ ls

#Data downloaded

*Comment for converting sra to Fastq in seperate folder*

#!/bin/bash

IN_DIR="/home/spiseq/SRA_data/sra/raw_V4/srr_v4_raw_files"
OUT_DIR="/home/spiseq/SRA_data/sra/raw_V4/srr_v4_fasta_files"

mkdir -p "$OUT_DIR"

for srafile in "$IN_DIR"/*.sra
do
    base=$(basename "$srafile" .sra)
    echo "Converting $base ..."
    fasterq-dump --split-files --outdir "$OUT_DIR" "$srafile"
done

echo "Conversion complete."

#To check which all files converted correctly containg 2 fastq files (paired end sra)

#!/bin/bash

FASTQ_DIR="/home/spiseq/SRA_data/sra/raw_V4/srr_v4_fasta_files"

cd "$FASTQ_DIR" || exit 1

for fq1 in *_1.fastq.gz
do
    base=${fq1%_1.fastq.gz}
    if [[ -f "${base}_2.fastq.gz" ]]; then
        echo "$base: Paired FASTQ files present"
    else
        echo "$base: Missing _2.fastq.gz file"
    fi
done

#output
SRR12597091: Missing _2.fastq.gz file
SRR12597092: Missing _2.fastq.gz file
SRR12597093: Missing _2.fastq.gz file
SRR12597098: Missing _2.fastq.gz file
SRR12597101: Missing _2.fastq.gz file
SRR12597102: Missing _2.fastq.gz file
SRR12597103: Missing _2.fastq.gz file
SRR12597104: Missing _2.fastq.gz file
SRR12597105: Missing _2.fastq.gz file
SRR12597106: Missing _2.fastq.gz file
SRR12597109: Missing _2.fastq.gz file
SRR12597110: Missing _2.fastq.gz file
SRR13483205: Paired FASTQ files present
SRR25744505: Paired FASTQ files present
SRR25744506: Paired FASTQ files present
SRR25744507: Paired FASTQ files present
SRR25744508: Paired FASTQ files present
SRR25744509: Paired FASTQ files present
SRR25744510: Paired FASTQ files present
SRR25744511: Paired FASTQ files present
SRR25744522: Paired FASTQ files present
SRR25744528: Paired FASTQ files present
SRR25744529: Paired FASTQ files present
SRR25744530: Paired FASTQ files present
SRR25744531: Paired FASTQ files present
SRR25744532: Paired FASTQ files present
SRR25744533: Paired FASTQ files present
SRR25744534: Paired FASTQ files present

✅ Step 1: Download Miniconda
cd ~
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh

✅ Step 2: Install Miniconda
bash Miniconda3-latest-Linux-x86_64.sh

Step 1: Find where Miniconda is installed

Run:

ls /home/spiseq

You should see a folder like:

miniconda3

If that folder exists, then continue.

Step 2: Manually activate Conda one time

Run the following:

eval "$(/home/spiseq/miniconda3/bin/conda shell.bash hook)"

Now test:

conda --version

You should now see a version number.

Step 3: Permanently enable conda in your shell

Now run:

conda init bash

Then reload shell:

source ~/.bashrc

Test:

conda --version

Now conda should work normally every time.

✅ Step 1: Accept TOS

Run these two commands:

conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/main
conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/r

✅ Step 2: Try creating the environment again
conda create -n hb-dada2-ex-wf -y

✅ Step 1: Activate the new environment
conda activate hb-dada2-ex-wf

Step 2: Install cutadapt (for primer trimming)
conda install -c bioconda cutadapt -y

✅ Step 3: Install fastp (optional, but useful for QC trimming)
conda install -c bioconda fastp -y

Step 4: Install R + DADA2 + DECIPHER (for denoising & taxonomy)
conda install -c conda-forge -c bioconda r-base=4.3 bioconductor-dada2 bioconductor-decipher -y


This step will take some minutes, because it installs R and required libraries.

Once all done environment is ready

If you also want RStudio (optional, recommended):
conda install -c conda-forge rstudio -y

Next Collect Primer sequence used in the study (V3-V4) 
Find common consequence of the primers used 
✅ So our trimming primers will be:
Forward:
CCTACGGGNGGCWGCAG

Reverse (reverse-complement when trimming R2):
GGACTACHVGGGTWTCTAAT

✅ Create an output directory for trimmed reads

Run this once:

mkdir trimmed_fastq

✅ Run Cutadapt on ALL samples in a loop

Copy and paste exactly:

for sample in *_1.fastq.gz
do
    base=$(basename "$sample" _1.fastq.gz)
    cutadapt \
      -g ^CCTACGGGNGGCWGCAG \
      -g ^ACTCCTACGGGAGGCAGCAG \
      -g ^CCTAYGGGRBGCASCAG \
      -G ^ATTAGAWACCCBDGTAGTCC \
      --error-rate 0.12 \
      --minimum-length 100 \
      -o trimmed_fastq/${base}_1.trimmed.fastq.gz \
      -p trimmed_fastq/${base}_2.trimmed.fastq.gz \
      ${base}_1.fastq.gz ${base}_2.fastq.gz
done

✅ Install FastQC and MultiQC (correct way)
conda install -c bioconda fastqc multiqc -y

✅ Then run the QC commands again
cd ~/SRA_data/sra/raw_V3V4/srr_v3v4_fastq_files/trimmed_fastq

fastqc *.trimmed.fastq.gz -o ../fastqc_reports
conda install -c bioconpip install multiqc
conda install -c bioconda -c conda-forge fastqc -y
pip install multiqc

Then rerun the QC commands:
cd ~/SRA_data/sra/raw_V3V4/srr_v3v4_fastq_files/trimmed_fastq
fastqc *.trimmed.fastq.gz -o ../fastqc_reports


Summarize:
cd ..
multiqc fastqc_reports -o fastqc_reports


Open report:
xdg-open fastqc_reports/multiqc_report.html

To get overall quality report for truncation
library(dada2)
library(ShortRead)
library(Biostrings)

# Path to trimmed fastq folder
path <- "/home/spiseq/SRA_data/sra/raw_V3V4/srr_v3v4_fastq_files/trimmed_fastq"

fnFs <- sort(list.files(path, pattern="_1.trimmed.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_2.trimmed.fastq.gz", full.names = TRUE))

# Function: extract per-base median and q10 quality
get_qual <- function(files){
  q_list <- lapply(files, function(f){
    fq <- readFastq(f)
    q <- as(quality(fq), "matrix")   # Convert to numeric matrix
    apply(q, 2, mean, na.rm = TRUE)  # Mean quality per position
  })
  q_mat <- do.call(cbind, q_list)
  data.frame(
    pos = 1:nrow(q_mat),
    medianQ = apply(q_mat, 1, median),
    q10 = apply(q_mat, 1, quantile, probs = 0.10)
  )
}

qualF_summary <- get_qual(fnFs)
qualR_summary <- get_qual(fnRs)

write.csv(qualF_summary, "Forward_quality_summary.csv", row.names = FALSE)
write.csv(qualR_summary, "Reverse_quality_summary.csv", row.names = FALSE)

cat("✅ Done! Quality profiles saved:\n- Forward_quality_summary.csv\n- Reverse_quality_summary.csv\n")

library(dada2)

# Path to trimmed fastq folder
path <- "/home/spiseq/SRA_data/sra/raw_V3V4/srr_v3v4_fastq_files/trimmed_fastq"

# Forward and reverse reads
fnFs <- sort(list.files(path, pattern="_1.trimmed.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_2.trimmed.fastq.gz", full.names = TRUE))

# Output folder for filtered reads
filt_path <- file.path(path, "filtered")
if(!dir.exists(filt_path)) dir.create(filt_path)

filtFs <- file.path(filt_path, basename(fnFs))
filtRs <- file.path(filt_path, basename(fnRs))

Truncking the poor quality reads
out <- filterAndTrim(
  fnFs, filtFs,
  fnRs, filtRs,
  truncLen = c(280, 250),
  maxN = 0,             # No ambiguous bases
  maxEE = c(2, 2),      # Max expected errors
  truncQ = 2,           # Truncate reads at first base with Q <= 2
  rm.phix = TRUE,       # Remove PhiX contamination
  compress = TRUE,
  multithread = TRUE
)

# Summary of how many reads passed filtering
head(out)
